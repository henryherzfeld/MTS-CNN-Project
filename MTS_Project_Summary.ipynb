{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ncxFByiFe_n"
   },
   "source": [
    "# MLB The Show Project Progress Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Phw9mUJDG30Y"
   },
   "source": [
    "We are attempting to use Deep Learning to answer the following question: Can an AI bat in the video game MLB The Show at a level comparable to or better than a human player? First we must describe the problem.\n",
    "\n",
    "When batting in MLB The Show, the player pays attention to a box in the center of the screen. This box remains until the pitcher begins to wind up and release the ball. When he does, the box disappears. In order to succeed, the player must watch the path of the ball across the screen coming towards the screen and determine if the ball is going to land in the box or not. If it is not going to land in the box, then it is called a ball and the player should not swing. If it is, it is a strike and the player then needs to decide when to swing. If the player swings too early or too late, they may either miss the ball or hit it weakly. By swinging at the right pitch with the right timing, the player maximizes their chances of success.\n",
    "\n",
    "When the ball lands after it is finished traveling, the game draws a blue circle(representing ball) or a red circle (representing strike) in the location on the screen where the pitch landed. Through OpenCV's template matching we were able to use this information to autonomously label our initial data as balls or strikes.\n",
    "\n",
    "To summarize, in order to succeed, an agent must do the following:\n",
    "\n",
    "1. Predict whether the pitch is a ball or a strike. \n",
    "2. If it is a strike, the agent must decide when to swing. If it is not, the agent must not swing.\n",
    "3. When providing feedback on swing timing, the game provides the following information:\n",
    "  - Too Early\n",
    "  - Very Early\n",
    "  - Early\n",
    "  - Just Early\n",
    "  - Good\n",
    "  - Just Late\n",
    "  - Late\n",
    "  - Very Late\n",
    "  - Too Late\n",
    "\n",
    "The agent's goal should be to get as many of its swings as possible in the \"good\" category.\n",
    "\n",
    "We have first set our sights on accomplishing (1). We have yet to address (2) or (3).\n",
    "\n",
    "To accomplish (1), I screen recorded several videos of exhibition gameplay from the game in 1080p and 30fps using the PS4's capture feature. Then, we split each video into frames using the ffmpeg library. Henry then wrote a labeling script to determine:\n",
    "\n",
    "1. In which frames was a pitch being thrown? \n",
    "2. Were these pitches balls or strikes?\n",
    "\n",
    "We noticed that it takes each pitch between 14 to 16 frames to travel from the pitcher's glove to home plate. So, we determined that we would use 15 frames to represent a pitch. From this, we were able to extract 1080 pitches (457 balls, 623 strikes) for a total of 16,200 images. An example of a ball and a strike from our initial dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-K5bO4Vvz2GW",
    "outputId": "d90a0b6f-37f4-4c0c-ce1e-2e6743cf3391"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4aqG2dvIXAP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('drive/My Drive/MTS NN Project/Deep Learning Term Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qdhZGW0wRMhS",
    "outputId": "89ef4c5d-23f9-43d6-8d76-ade98d3ee69b"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "for root, dirs, files in os.walk('labeled/Balls/ball1'):\n",
    "  for i, file in enumerate(files):\n",
    "    path = os.path.join(root, file) \n",
    "    img=cv2.imread(path)\n",
    "    cv2_imshow(img)\n",
    "    #print(file)\n",
    "\n",
    "for root, dirs, files in os.walk('labeled/Strikes/strike1'):\n",
    "  for i, file in enumerate(files):\n",
    "    path = os.path.join(root, file) \n",
    "    img=cv2.imread(path)\n",
    "    cv2_imshow(img)\n",
    "    #print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAN3BjHw0j0L"
   },
   "source": [
    "It is easy to see that all the information in these 1920 x 1080 images is not necessary to make a ball vs strike classification. The only thing that is really necesary to keep is the strike zone and the area around it. So, we cropped our data, resulting in 920x880 images. Examples are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1I85rBKAYKT4",
    "outputId": "0c4687a2-53d6-4730-cbef-3452ad9d99fd"
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk('cropped_labeled/Balls/ball1'):\n",
    "  for i, file in enumerate(files):\n",
    "    path = os.path.join(root, file) \n",
    "    img=cv2.imread(path)\n",
    "    cv2_imshow(img)\n",
    "    #print(file)\n",
    "\n",
    "for root, dirs, files in os.walk('cropped_labeled/Strikes/strike1'):\n",
    "  for i, file in enumerate(files):\n",
    "    path = os.path.join(root, file) \n",
    "    img=cv2.imread(path)\n",
    "    cv2_imshow(img)\n",
    "    #print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J91N8k5_Ye-_"
   },
   "source": [
    "An input to any well known CNN architecture of 920 x 880 RGB images would certainly be computationally expensive. So, we reduced our images further to 115 x 110. Since the ball is white, we were also able to make the images grayscale. These resulted in the examples shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S8mtB8hhZYAz",
    "outputId": "dc80de3c-b097-4e49-9bfc-b9a225df8353"
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk('labeled_resized/Balls/ball1'):\n",
    "  for i, file in enumerate(files):\n",
    "    path = os.path.join(root, file) \n",
    "    img=cv2.imread(path)\n",
    "    cv2_imshow(img)\n",
    "    #print(file)\n",
    "\n",
    "for root, dirs, files in os.walk('labeled_resized/Strikes/strike1'):\n",
    "  for i, file in enumerate(files):\n",
    "    path = os.path.join(root, file) \n",
    "    img=cv2.imread(path)\n",
    "    cv2_imshow(img)\n",
    "    #print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnGNfgPLZdTX"
   },
   "source": [
    "Since grayscale images have only one color channel, we chose to stack the 15 frames to represent a pitch on top of each other as color channels. This will represent a single instance of our data.\n",
    "\n",
    "#Frame Differencing:\n",
    "\n",
    "The final input to our neural network is an array of frame difference data between the 15 frames stacked together as a single pitch, since this will highlight only the difference in the position of the ball from one frame to the next, which is what we wish to track when making a ball vs strike classification. OpenCV's [absdiff](https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#absdiff) method was used to calculate frame differences, as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A1Qzrxv5-6-9",
    "outputId": "94e03677-55c2-411c-bd15-2d39d6583e00"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_frame_differences(frames, thresholding=False):\n",
    "  n = len(frames)\n",
    "\n",
    "  first_frame = frames[0]\n",
    "  acc = np.empty((n-1, *first_frame.shape))\n",
    "\n",
    "  for i, frame in enumerate(frames[1:]):\n",
    "    diff = cv2.absdiff(first_frame, frame)\n",
    "\n",
    "    if thresholding:\n",
    "      ret, diff = cv2.threshold(diff, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    acc[i] = diff\n",
    "    first_frame = frame\n",
    "\n",
    "  return acc\n",
    "\n",
    "path = 'labeled_resized/Strikes/strike4/'\n",
    "files = [cv2.imread(path + file) for file in sorted(os.listdir(path))]\n",
    "\n",
    "diffs = get_frame_differences(files, False);\n",
    "\n",
    "for im in diffs:\n",
    "  cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMyvCvMe-7l8"
   },
   "source": [
    "These frame differences images are fed to a convolutional neural network. Currently, we are testing different CNN architectures such as VGG and ResNet to see which of them produces the best test accuracy on this data. We hope to eventually publish these results in a paper with Hill. He is very interested in this because the dataset is so unique.\n",
    "\n",
    "To reiterate, our agent's goals are:\n",
    "\n",
    "1. Predict whether the pitch is a ball or a strike. \n",
    "2. If it is a strike, the agent must decide when to swing. If it is not, the agent must not swing.\n",
    "3. When providing feedback on swing timing, the game provides the following information:\n",
    "  - Too Early\n",
    "  - Very Early\n",
    "  - Early\n",
    "  - Just Early\n",
    "  - Good\n",
    "  - Just Late\n",
    "  - Late\n",
    "  - Very Late\n",
    "  - Too Late\n",
    "\n",
    "\n",
    "Future work may involve using reinforcement learning to address (2) and (3), using a reinforcement learning approach for the whole task, or using early prediction to address the timing issue described in (2), and combining the classification networks from (1) and (2) to address (3) using reinforcement learning. This is what we would like advice on."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MTS Project Summary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
